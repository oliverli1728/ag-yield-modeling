{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from yellowbrick.model_selection import ValidationCurve, LearningCurve\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import xgboost as xgb\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "import heapq\n",
    "import math \n",
    "import statistics \n",
    "from nasspython.nass_api import nass_data\n",
    "from NdviApi import NDVI\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Helpful Links:\n",
    "https://geo.fas.usda.gov/GADAS/index.html#\n",
    "https://glam1.gsfc.nasa.gov/\n",
    "https://www.mdpi.com/2072-4292/13/21/4227\n",
    "https://glam1.gsfc.nasa.gov/api/doc/db/versions#default-db\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generic function to initialize dataframes from USDA Quickstats data\n",
    "def initialize_df(states, week, start_yr, end_yr, freq, suffix, var):\n",
    "    loop = True \n",
    "    while loop:\n",
    "        try:\n",
    "            temp = pd.DataFrame(nass_data(\"B3744D45-DFBE-3B88-AFB1-25CBC8E64550\", agg_level_desc=\"STATE\", freq_desc=freq, short_desc=var)['data'])\n",
    "            national_data = pd.DataFrame(nass_data(\"B3744D45-DFBE-3B88-AFB1-25CBC8E64550\", agg_level_desc=\"NATIONAL\", freq_desc=freq, short_desc=var)['data'])\n",
    "            loop = False\n",
    "        except ValueError as e:\n",
    "            continue\n",
    "\n",
    "    temp = pd.concat([temp, national_data], axis=0)\n",
    "    temp = temp[temp[\"state_name\"].isin(states)]\n",
    "    if (freq == \"WEEKLY\"):\n",
    "        temp = temp[temp[\"reference_period_desc\"].eq(f\"WEEK #{week}\")]\n",
    "    elif (freq == \"ANNUAL\"):\n",
    "        temp = temp[temp[\"reference_period_desc\"].eq(\"YEAR\")]\n",
    "    temp = temp[temp[\"year\"].ge(start_yr) & temp[\"year\"].le(end_yr)]\n",
    "    temp = temp.pivot(index='year', columns='state_name', values=\"Value\")\n",
    "    temp = temp.add_suffix(suffix)\n",
    "    temp = temp.replace(\",\", \"\", regex=True)\n",
    "    temp = temp.fillna(0).astype(float)\n",
    "    return temp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NDVI data via https://glam1.gsfc.nasa.gov/\n",
    "def get_ndvi_data(states, start_yr, end_yr, ids, mask, threshold, index, forecast=False):\n",
    "    acc_ndvi_df = pd.DataFrame(index=index)\n",
    "    for i in range(len(ids)):\n",
    "        acc_ndvi = list()\n",
    "        df = NDVI().get_data(\n",
    "                            version='v15', \n",
    "                            sat='MOD',\n",
    "                            mask=mask, \n",
    "                            shape='ADM',\n",
    "                            start_yr=start_yr,\n",
    "                            end_yr=end_yr,\n",
    "                            start_month=1,\n",
    "                            num_months=12,\n",
    "                            ids=ids[i],\n",
    "                            ts_type='cumulative',\n",
    "                            mcv=0.0\n",
    "                            )\n",
    "        count=0\n",
    "        df.reset_index(inplace=True)\n",
    "        for j in range(df.shape[0]):\n",
    "            if not forecast:\n",
    "                if (int(df.loc[j, \"ORDINAL DATE\"][-3:]) == 329):\n",
    "                        acc_ndvi.append(count)\n",
    "                        count=0\n",
    "                elif (df.loc[j, \"SAMPLE VALUE\"] >= threshold): \n",
    "                        count+=df.loc[j, \"SAMPLE VALUE\"] \n",
    "            else: \n",
    "                if df.loc[j, \"SAMPLE VALUE\"] >= threshold:\n",
    "                    count+=df.loc[j, \"SAMPLE VALUE\"] \n",
    "        \n",
    "        if forecast:\n",
    "            acc_ndvi.append(count)\n",
    "            \n",
    "        acc_ndvi_df[states[i] + \" NDVI\"] = acc_ndvi  \n",
    "                \n",
    "    return acc_ndvi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drought index data from https://droughtmonitor.unl.edu/About.aspx\n",
    "def get_drought_data(states, ids, start_yr, end_yr, index):\n",
    "    drought_data = pd.DataFrame(index=index)\n",
    "    for i in range (len(ids)):\n",
    "        if len(str(ids[i])) == 1:\n",
    "            df = pd.read_json(f\"https://usdmdataservices.unl.edu/api/StateStatistics/GetDroughtSeverityStatisticsByArea?aoi=0{ids[i]}&startdate=1/1/{start_yr}&enddate=1/1/{end_yr+1}&statisticsType=1\")\n",
    "        else:\n",
    "            df = pd.read_json(f\"https://usdmdataservices.unl.edu/api/StateStatistics/GetDroughtSeverityStatisticsByArea?aoi={ids[i]}&startdate=1/1/{start_yr}&enddate=1/1/{end_yr+1}&statisticsType=1\")\n",
    "        df = df.loc[:, [\"ValidStart\", \"None\", \"D0\", \"D1\", \"D2\", \"D3\", \"D4\"]]\n",
    "        df[\"ValidStart\"] = pd.to_datetime(df[\"ValidStart\"]).dt.to_period('Y')\n",
    "        df = df.iloc[::-1].set_index(\"ValidStart\", drop=True)\n",
    "        df = df.replace(\",\", \"\", regex=True)\n",
    "        df = df.fillna(0).astype(float)\n",
    "        df = df.groupby(lambda x: x.year)[[\"None\", \"D0\", \"D1\", \"D2\", \"D3\", \"D4\"]].mean()\n",
    "        df.index.name = None\n",
    "        df = df.add_prefix(f\"{states[i]} \")\n",
    "        drought_data = pd.concat([drought_data, df], axis=1)\n",
    "    return drought_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to initialize data\n",
    "\n",
    "Pass crop, week to take data from, and array of 1s and 0s indicating which additional variables to include.\n",
    "\n",
    "Available crops are \"CORN\", \"WHEAT, WINTER\" and \"SOYBEAN\"\n",
    "\n",
    "Below are the available variables, in order:\n",
    "1. PCT Excellent at current week\n",
    "2. PCT Good at current week\n",
    "3. PCT Change in Excellent (pct @ week - pct @ week - 10)\n",
    "4. PCT Change in Good (pct @ week - pct @ week - 10)\n",
    "5. Acres planted/year\n",
    "6. Acres harvested at current week\n",
    "7. PCT Mature at current week\n",
    "8. Drought data\n",
    "9. PCT Fair at current week\n",
    "10. PCT Poor at current week\n",
    "11. PCT Very Poor at current week\n",
    "\n",
    "\n",
    "Sample input below\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_data(crop, week, start_yr=2000, end_yr=2023, var_arr=[], forecast=False):\n",
    "    \n",
    "    if (crop == \"CORN\"):\n",
    "        mask = \"USDA-NASS-CDL_2018-2023_corn-50pp\"\n",
    "        states = [\"US TOTAL\", \"WISCONSIN\", \"SOUTH DAKOTA\", \"OHIO\", \"NEBRASKA\", \"MISSOURI\", \"MINNESOTA\", \"KANSAS\", \"IOWA\", \"INDIANA\", \"ILLINOIS\"]\n",
    "        ndvi_ids = [27258, 26264, 26237, 26258, 26228, 26253, 26251, 26226, 26246, 26245, 26244]\n",
    "        drought_ids = [55, 46, 39, 31, 29, 27, 20, 19, 18, 17]\n",
    "        threshold = 0.58\n",
    "    elif (crop == \"SOYBEANS\"):\n",
    "        mask = \"USDA-NASS-CDL_2018-2023_soybean-50pp\"\n",
    "        states = [\"US TOTAL\", \"SOUTH DAKOTA\", \"OHIO\", \"NORTH DAKOTA\", \"NEBRASKA\", \"MISSOURI\", \"MINNESOTA\", \"IOWA\", \"INDIANA\", \"ILLINOIS\", \"ARKANSAS\"]\n",
    "        ndvi_ids = [27258, 26237, 26258, 26236, 26228, 26253, 26251, 26246, 26245, 26244, 26240]\n",
    "        drought_ids = [46, 39, 38, 31, 29, 27, 19, 18, 17, 5]\n",
    "        threshold = 0.58\n",
    "    elif (crop == \"WHEAT, WINTER\"):\n",
    "        mask = \"USDA-NASS-CDL_2018-2023_winterwheat-50pp\"\n",
    "        states = [\"US TOTAL\", \"WASHINGTON\", \"SOUTH DAKOTA\", \"OKLAHOMA\", \"NEBRASKA\", \"MONTANA\", \"MISSOURI\", \"KANSAS\", \"ILLINOIS\", \"IDAHO\", \"COLORADO\"]\n",
    "        ndvi_ids = [27258, 26234, 26237, 26230, 26228, 26227, 26253, 26226, 26244, 26225, 26224]\n",
    "        drought_ids = [53, 46, 40, 31, 30, 29, 20, 17, 16, 8]\n",
    "        threshold = 0.34\n",
    "    \n",
    "    idx = list(range(start_yr, end_yr + 1))\n",
    "    \"\"\"\n",
    "    Get crop mask from: https://glam1.gsfc.nasa.gov/api/doc/db/versions#default-db\n",
    "    \"\"\"\n",
    "    df = get_ndvi_data(states, start_yr, end_yr, ndvi_ids, mask=mask, threshold=threshold, index=idx, forecast=forecast)\n",
    "    df.index.name = 'year'\n",
    "\n",
    "    if not forecast:\n",
    "        if (crop == \"CORN\"):\n",
    "            yields = initialize_df(states, None, start_yr, end_yr, \"ANNUAL\", \"\", \"CORN, GRAIN - YIELD, MEASURED IN BU / ACRE\")\n",
    "        else:\n",
    "            yields = initialize_df(states, None, start_yr, end_yr, \"ANNUAL\", \"\", f\"{crop} - YIELD, MEASURED IN BU / ACRE\")\n",
    "\n",
    "    overlay = pd.DataFrame()\n",
    "    \n",
    "    additional_vars = {\n",
    "                    0: f\"{crop} - CONDITION, MEASURED IN PCT EXCELLENT\",\n",
    "                    1: f\"{crop} - CONDITION, MEASURED IN PCT GOOD\",\n",
    "                    2: f\"{crop} - CONDITION, MEASURED IN PCT EXCELLENT\",\n",
    "                    3: f\"{crop} - CONDITION, MEASURED IN PCT GOOD\",\n",
    "                    4: \"PCT CHNGE EXCELLENT\",\n",
    "                    5: \"PCT CHNGE GOOD\",\n",
    "                    6: f\"{crop} - ACRES PLANTED\",\n",
    "                    7: f\"{crop} - PROGRESS, MEASURED IN PCT HARVESTED\",\n",
    "                    8: f\"{crop} - PROGRESS, MEASURED IN PCT EMERGED\",\n",
    "\n",
    "                    # Drought data not working atm\n",
    "                    9: \"Drought data\" ,\n",
    "                    10: f\"{crop} - CONDITION, MEASURED IN PCT FAIR\",\n",
    "                    11: f\"{crop} - CONDITION, MEASURED IN PCT POOR\",\n",
    "                    12: f\"{crop} - CONDITION, MEASURED IN PCT VERY POOR\",\n",
    "    }\n",
    "\n",
    "    for i in range(11):\n",
    "        if var_arr[i] == 1:\n",
    "            if i == 2:\n",
    "                excellent_prev = initialize_df(states, week - 5, start_yr, end_yr, \"WEEKLY\", \" EXCLNT CHNGE\", additional_vars[i])\n",
    "                excellent_curr = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" EXCLNT CHNGE\", additional_vars[i])\n",
    "                pct_chnge_excellent = excellent_curr - excellent_prev\n",
    "                overlay = pd.concat([overlay, pct_chnge_excellent], axis=1)\n",
    "            elif i == 3:\n",
    "                good_prev = initialize_df(states, week - 5, start_yr, end_yr, \"WEEKLY\", \" GD CHNGE\", additional_vars[i])\n",
    "                good_curr = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" GD CHNGE\", additional_vars[i])\n",
    "                pct_chnge_good = good_curr - good_prev\n",
    "                overlay = pd.concat([overlay, pct_chnge_good], axis=1)\n",
    "            elif i == 4:\n",
    "                planted = initialize_df(states, None, start_yr, end_yr, \"ANNUAL\", \" ACRES PLANTED\", additional_vars[6])\n",
    "                overlay = pd.concat([overlay, planted], axis=1)\n",
    "            elif i == 5:\n",
    "                if (crop == \"CORN\"):\n",
    "                    harvested = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" ACRES HARVESTED\", \"CORN, GRAIN - PROGRESS, MEASURED IN PCT HARVESTED\")\n",
    "                else:\n",
    "                    harvested = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" ACRES HARVESTED\", additional_vars[7])\n",
    "                overlay = pd.concat([overlay, harvested], axis=1)\n",
    "            elif i == 6:\n",
    "                if crop == \"CORN\":\n",
    "                    var = f\"{crop} - PROGRESS, MEASURED IN PCT SILKING\"\n",
    "                    maturity = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" PROGRESS\", var)\n",
    "                else:\n",
    "                    maturity = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" PROGRESS\", additional_vars[8])\n",
    "                overlay = pd.concat([overlay, maturity], axis=1)\n",
    "            elif i == 7:\n",
    "                drought_index = get_drought_data(states, drought_ids, start_yr, end_yr, idx)\n",
    "                overlay = pd.concat([overlay, drought_index], axis=1)\n",
    "            elif i == 8:\n",
    "                fair = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" PCT FAIR\", additional_vars[10])\n",
    "                overlay = pd.concat([overlay, fair], axis=1)\n",
    "            elif i == 9:\n",
    "                poor = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" PCT POOR\", additional_vars[11])\n",
    "                overlay = pd.concat([overlay, poor], axis=1)\n",
    "            elif i == 10:\n",
    "                very_poor = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" PCT VERY POOR\", additional_vars[12])\n",
    "                overlay = pd.concat([overlay, very_poor], axis=1)\n",
    "            elif i == 0:\n",
    "                temp = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" PCT EXCELLENT\", additional_vars[i])\n",
    "                overlay = pd.concat([overlay, temp], axis=1)   \n",
    "            elif i == 1:\n",
    "                temp = initialize_df(states, week, start_yr, end_yr, \"WEEKLY\", \" PCT GOOD\", additional_vars[i])\n",
    "                overlay = pd.concat([overlay, temp], axis=1)\n",
    "                \n",
    "    x = pd.concat([df, overlay], axis=1)\n",
    "    x.fillna(0, inplace=True)\n",
    "    x.head()\n",
    "    if not forecast:\n",
    "        return yields, x\n",
    "    else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairplots():\n",
    "    global x \n",
    "    global y \n",
    "    for state in y.columns:\n",
    "        state_data = x.filter(regex=state)\n",
    "        temp = pd.DataFrame(y[state], columns=[state])\n",
    "        temp.reset_index(drop=True, inplace=True)\n",
    "        g = sns.pairplot(state_data.assign(yields=temp[state]), y_vars=['yields'][:], x_vars=state_data.columns)\n",
    "        sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional filtering based on Spearman correlation\n",
    "\n",
    "def spearman_correlations(filter=True):\n",
    "    global x \n",
    "    global y \n",
    "    fig, axs = plt.subplots(5, 2, figsize=(18, 18))\n",
    "\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        state = y.columns[i-1]\n",
    "        corr = x.filter(regex=state).apply(lambda x: spearmanr(x, y[state])[0])\n",
    "        if filter:\n",
    "            temp = abs(corr)\n",
    "            x.drop(columns=temp.nsmallest(math.floor(len(corr.index) / 2)).index, inplace=True)\n",
    "        ax.barh(y=corr.index.str.lstrip(state), width=corr.sort_values(), color='lightsteelblue')\n",
    "        ax.set_title(state);\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional filtering based on mutual information\n",
    "def mutual_information(filter=True):\n",
    "    fig, axs = plt.subplots(5, 2, figsize=(18, 18))\n",
    "\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        state = y.columns[i-1]\n",
    "        state_data = x.filter(regex=state)\n",
    "        mi = mutual_info_regression(X=state_data,\n",
    "                                    y=y[state],\n",
    "                                    random_state=42\n",
    "                                    )\n",
    "        mutual_info = pd.DataFrame(mi, index=state_data.columns, columns=[state])\n",
    "        if filter:\n",
    "            x.drop(columns=mutual_info[state].nsmallest(math.floor(len(mutual_info.index) / 2)).index, inplace=True)\n",
    "        ax.barh(state_data.columns, mutual_info[state])\n",
    "        ax.set_title(state)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Returns feature coefficients of regression model\n",
    "def feature_importance(model, df, ax, num_features, plot=False, print_results=False):\n",
    "        importance = model.coef_\n",
    "        keys = list(df.keys())\n",
    "        abs_weights = {}\n",
    "        weights = {}\n",
    "        for i,v in enumerate(importance):\n",
    "            if print_results:\n",
    "                print(\"Feature: %s, Score: %.5f\" % (keys[i],v))\n",
    "            abs_weights[keys[i]] = abs(v)\n",
    "            weights[keys[i]] = v\n",
    "        if plot:\n",
    "            ax.bar([x for x in range(len(importance))], height=importance, color='b')\n",
    "            tickvals = range(0, len(importance))\n",
    "            cols = df.columns\n",
    "            ax.set_xticks(ticks=tickvals, labels=cols, rotation=45, fontsize='xx-small', fontstretch='extra-condensed')\n",
    "        \n",
    "        largest_features = heapq.nlargest(num_features, abs_weights, key=abs_weights.get)\n",
    "\n",
    "        avg_abs_weight = sum([abs_weights.get(key) for key in largest_features]) / len(largest_features)\n",
    "        return largest_features, weights, avg_abs_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lasso and xgb finetuning\n",
    "\n",
    "def get_best_model(X, y, kwargs=None, lasso=True, print_results=False, scoring=None, cv=5):\n",
    "    xgb_params = {\n",
    "    \"learning_rate\": np.arange(0.01, 0.2, 0.01),\n",
    "    \"min_child_weight\": np.arange(1, 5, 1),\n",
    "    \"n_estimators\": range(800, 1200),\n",
    "    \"max_depth\": range(1, 5),\n",
    "    \"colsample_bytree\": np.arange(0.1, 1, 0.1),\n",
    "    \"subsample\": np.arange(0.1, 1, 0.1)\n",
    "    }\n",
    "\n",
    "    lasso_params = {\n",
    "        \"alpha\": np.arange(0.05, 2, 0.05)\n",
    "    }\n",
    "\n",
    "    if not lasso:\n",
    "        optimized_model = RandomizedSearchCV(param_distributions=xgb_params, estimator=xgb.XGBRegressor(), scoring=scoring, verbose=1, random_state=42, cv=2)\n",
    "        optimized_model.fit(X, y)\n",
    "        if print_results:\n",
    "            print(\"Best Parameters:\", optimized_model.best_params_)\n",
    "    else: \n",
    "        optimized_model = RandomizedSearchCV(param_distributions=lasso_params, estimator=Lasso(random_state=42), scoring=scoring, verbose=1, random_state=42, cv=cv)\n",
    "        optimized_model.fit(X, y)\n",
    "        if print_results:\n",
    "            print(\"Best Parameters:\", optimized_model.best_params_)\n",
    "    return optimized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting models, analyzing and plotting data\n",
    "\n",
    "def reg_plot(x, y, title, ax=None, forecast=False, x_forecast=None, model_df=None):\n",
    "\n",
    "    if forecast:\n",
    "        x_forecast = x_forecast.filter(axis=1, items=x.columns)\n",
    "        for i in x.columns:\n",
    "            if i not in x_forecast.columns:\n",
    "                x.drop(columns=[i], inplace=True)\n",
    "    \n",
    "    models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso' : Lasso(alpha = 0.45, random_state=42),\n",
    "    'Ridge' : Ridge(random_state=42),\n",
    "    'Decision Tree': DecisionTreeRegressor(max_depth=3, random_state=42),\n",
    "    'KNN': KNeighborsRegressor(),\n",
    "    'XGB': xgb.XGBRegressor(n_estimators=1000,\n",
    "                            max_depth=3,\n",
    "                            gamma = 5,\n",
    "                            subsample = 0.5,\n",
    "                            random_state=42),\n",
    "    'XGBRF': xgb.XGBRFRegressor(n_estimators=1000, \n",
    "                                subsample = 0.5,\n",
    "                                random_state=42),\n",
    "    'RF Regressor': RandomForestRegressor(\n",
    "                                        random_state=42)\n",
    "    }\n",
    "    \n",
    "    r2_scores = {}\n",
    "    maes = {}\n",
    "    print(title)\n",
    "    for i in range(25):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = i, shuffle=True)\n",
    "        for name, md in models.items():\n",
    "            md.fit(X_train,y_train)\n",
    "            y_pred = md.predict(X_test)\n",
    "            print(f\"{name}: mae : {mean_absolute_error(y_test, y_pred)} score : {r2_score(y_test, y_pred)}\")\n",
    "            if i == 0:\n",
    "                maes[md] = mean_absolute_error(y_test, y_pred)\n",
    "                r2_scores[md] = r2_score(y_test, y_pred)\n",
    "            else:\n",
    "                maes[md] = maes.get(md) + mean_absolute_error(y_test, y_pred)\n",
    "                r2_scores[md] = r2_scores.get(md) + r2_score(y_test, y_pred)\n",
    "            model_df.loc[title, name] = r2_score(y_test, y_pred)\n",
    "    print(\"-------------\")\n",
    "\n",
    "    for md in r2_scores.keys():\n",
    "        r2_scores[md] = r2_scores.get(md) / 25\n",
    "        maes[md] = maes.get(md) / 25\n",
    "    model = max(r2_scores, key=lambda x:r2_scores[x])\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_train_pred = pd.DataFrame(model.predict(X_train), index=X_train.index)\n",
    "    y_test_pred = pd.DataFrame(model.predict(X_test), index=X_test.index)\n",
    "\n",
    "    if forecast:\n",
    "        y_forecast = model.predict(x_forecast)\n",
    "        ax.scatter(24, y_forecast, color='purple')\n",
    "\n",
    "    combinedX = pd.concat([X_train, X_test], axis=0).sort_index()\n",
    "    combinedY = pd.concat([y_train, y_test], axis=0).sort_index()\n",
    "    combinedY_pred = pd.concat([y_train_pred, y_test_pred], axis=0).sort_index()\n",
    "    ax.plot(combinedX.index, combinedY, label='Actual Yield')\n",
    "    ax.plot(combinedX.index, combinedY_pred, label='Predicted Yield')\n",
    "    ax.scatter(X_test.index, y_test_pred, label='Out of Sample Predictions', color='black')\n",
    "    ax.legend(loc='lower right', fontsize='small')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "\n",
    "    s = r2_scores[model]\n",
    "    r = maes[model]\n",
    "    best_model = [i for i in models if models[i] == model]\n",
    "    at = AnchoredText(\n",
    "        f\"OOS AVG R2: {s:.2f}\\nOOS AVG MAE {r:.2f}\\nModel: \" + best_model[0],\n",
    "        prop=dict(size=\"medium\"),\n",
    "        frameon=True,\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "    at.patch.set_boxstyle(\"square, pad=0.0\")\n",
    "    ax.add_artist(at)\n",
    "    if forecast:\n",
    "        return ax, s, y_forecast, model_df\n",
    "    else:\n",
    "        return ax, s, model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, pred):\n",
    "    return np.sqrt(mean_squared_error(y_true=y_true, y_pred=pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_val_curves(x, y, ax):\n",
    "    X_train, _, y_train, _ = train_test_split(x,y, test_size = 0.1, random_state = 42, shuffle=True)\n",
    "\n",
    "    rmse_score = make_scorer(rmse)\n",
    "    cv_rmse = {}\n",
    "    alphas = list(np.arange(0.1, 3.0, 0.1))\n",
    "    for alpha in alphas:\n",
    "        pipe = Pipeline([('scaler', MinMaxScaler()),\n",
    "                        ('lasso', Lasso(alpha=alpha))])\n",
    "        cv_rmse[alpha] = cross_val_score(pipe,\n",
    "                                        X=X_train,\n",
    "                                        y=y_train,\n",
    "                                        scoring=rmse_score,\n",
    "                                        cv=5)\n",
    "\n",
    "\n",
    "    cv_rmse = pd.DataFrame.from_dict(cv_rmse, orient='index')\n",
    "    best_alpha, best_rmse = cv_rmse.mean(1).idxmin(), cv_rmse.mean(1).min()\n",
    "    cv_rmse = cv_rmse.stack().reset_index()\n",
    "    cv_rmse.columns=['alpha', 'iter', 'RMSE']\n",
    "    ax = sns.lineplot(x='alpha', y='RMSE', data=cv_rmse, ax=ax)\n",
    "    ax.set_title(f'Cross-Validation Results Lasso | Best alpha: {best_alpha} | Best RMSE: {best_rmse:.2f}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_curves():\n",
    "    fig, axs = plt.subplots(5, 2, figsize=(18, 18))\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        state = y.columns[i]\n",
    "        cross_val_curves(x.filter(regex=state), y[state], ax=ax)\n",
    "    plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 2, figsize=(20, 20))\n",
    "fig.delaxes(axs[5, 1])\n",
    "\n",
    "r2_by_state = {}\n",
    "cumulative_r2 = {}\n",
    "y, x = get_data(\"WHEAT, WINTER\", 23, 2000, 2023, [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
    "model_df = pd.DataFrame(index=y.columns, columns=[\"Linear Regression\", \"Lasso\", \"Ridge\", \"Decision Tree\", \"KNN\", \"XGB\", \"XGBRF\", \"RF Regressor\"])\n",
    "preds = pd.DataFrame(index=[\"WHEAT, WINTER\"], columns = y.columns)\n",
    "mutual_information(filter=True)\n",
    "for i, ax in enumerate(fig.axes):\n",
    "        col_name = y.columns[i]\n",
    "        input = x.filter(regex=col_name)\n",
    "        x_forecast = get_data(\"WHEAT, WINTER\", 23, 2024, 2024, [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], forecast=True)\n",
    "        _, r2_by_state[col_name], y_forecast, model_df = reg_plot(input, y[col_name], col_name, ax, forecast=True, x_forecast=x_forecast.filter(regex=col_name), model_df=model_df)\n",
    "        preds.loc[\"WHEAT, WINTER\", col_name] = y_forecast\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.show()\n",
    "\n",
    "preds.to_csv(\"winter wheat 2024.csv\")\n",
    "model_df.to_csv(\"winter wheat models r2.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 2, figsize=(18, 18))\n",
    "fig.delaxes(axs[5, 1])\n",
    "\n",
    "r2_by_state = {}\n",
    "cumulative_r2 = {}\n",
    "\n",
    "\n",
    "\n",
    "y, x = get_data(\"CORN\", 30, 2000, 2023, [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
    "model_df = pd.DataFrame(index=y.columns, columns=[\"Linear Regression\", \"Lasso\", \"Ridge\", \"Decision Tree\", \"KNN\", \"XGB\", \"XGBRF\", \"RF Regressor\"])\n",
    "preds = pd.DataFrame(index=[\"CORN\"], columns=y.columns)\n",
    "mutual_information(filter=True)\n",
    "for i, ax in enumerate(fig.axes):\n",
    "        if i <= 9:\n",
    "                col_name = y.columns[i]\n",
    "                input = x.filter(regex=col_name)\n",
    "                x_forecast = get_data(\"CORN\", 30, 2024, 2024, [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], forecast=True)\n",
    "                _, r2_by_state[col_name], y_forecast, model_df= reg_plot(input, y[col_name], col_name, ax, forecast=True, x_forecast=x_forecast.filter(regex=col_name), model_df=model_df)\n",
    "                preds.loc[\"CORN\", col_name] = y_forecast\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.show()\n",
    "\n",
    "preds.to_csv(\"corn 2024.csv\")\n",
    "model_df.to_csv(\"corn models r2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, 2, figsize=(18, 18))\n",
    "fig.delaxes(axs[5, 1])\n",
    "\n",
    "r2_by_state = {}\n",
    "cumulative_r2 = {}\n",
    "y, x = get_data(\"SOYBEANS\", 30, 2000, 2023, [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
    "model_df = pd.DataFrame(index=y.columns, columns=[\"Linear Regression\", \"Lasso\", \"Ridge\", \"Decision Tree\", \"KNN\", \"XGB\", \"XGBRF\", \"RF Regressor\"])\n",
    "preds = pd.DataFrame(index=[\"SOYBEANS\"], columns=y.columns)\n",
    "mutual_information(filter=True)\n",
    "for i, ax in enumerate(fig.axes):\n",
    "        if i <= 9:\n",
    "                col_name = y.columns[i]\n",
    "                input = x.filter(regex=col_name)\n",
    "                x_forecast = get_data(\"SOYBEANS\", 30, 2024, 2024, [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], forecast=True)\n",
    "                _, r2_by_state[col_name], y_forecast, model_df = reg_plot(input, y[col_name], col_name, ax, forecast=True, x_forecast=x_forecast.filter(regex=col_name), model_df=model_df)\n",
    "                preds.loc[\"SOYBEANS\", col_name] = y_forecast\n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "plt.show()\n",
    "\n",
    "preds.to_csv(\"soybeans 2024.csv\")\n",
    "model_df.to_csv(\"soybean models r2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
